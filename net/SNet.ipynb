{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ad09ae",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2890b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/SoyNet\"\n",
    "tensorboard_path = \"./tensorboard/SoyNet\"\n",
    "models_path = \"./models/SoyNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31811432",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3073bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def my_tensor_image_show ( image , label=None ):\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    if label is None :\n",
    "        plt.title('Image in tensor format.')\n",
    "    else :\n",
    "        plt.title(f'Image in tensor format | Class: {label:2d}')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega a ResNet do PyTorch\n",
    "\n",
    "import torch\n",
    "from torchvision.models import ResNet, ResNet18_Weights, resnet18\n",
    "\n",
    "\n",
    "# Transformações para a ResNet18 \n",
    "my_transform = ResNet18_Weights.DEFAULT.transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "# Função para dividir o dataset em treino e teste\n",
    "def train_test_dataset(dataset, test_split=0.25):\n",
    "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split)\n",
    "    train_data = Subset(dataset, train_idx)\n",
    "    test_data = Subset(dataset, test_idx)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b653c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "# Carrega o dataset de imagens com as transformações definidas\n",
    "data = ImageFolder(root=dataset_path, transform=my_transform)\n",
    "\n",
    "# Divide o dataset em treino e teste\n",
    "train_data, test_data = train_test_dataset(data, 0.30)\n",
    "\n",
    "# Cria DataLoaders para treino e teste\n",
    "batch_size = 512\n",
    "\n",
    "train_tensors = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_tensors = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0599b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_tensors))\n",
    "my_tensor_image_show(images[0], label=labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164548f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_tensors))\n",
    "my_tensor_image_show(images[0], label=labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d2424",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67518f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate ( model , data , device='cpu') :\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    sum = 0\n",
    "    \n",
    "    for idx, (test_x, test_label) in enumerate(data) : \n",
    "        test_x = test_x.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "        predict_y = model( test_x ).detach()\n",
    "        predict_ys = torch.max( predict_y, axis=1 )[1]\n",
    "        sum = sum + test_x.size(0)\n",
    "        correct = correct + torch.sum(predict_ys == test_label)\n",
    "        correct = correct.cpu().item()\n",
    "    \n",
    "    return correct*100./sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.optim \n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "\n",
    "def plot_layers ( net , writer, epoch ) :\n",
    "    layers = list(net.classifier.modules())\n",
    "    \n",
    "    layer_id = 1\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, torch.nn.Linear) :\n",
    "\n",
    "#             writer.add_histogram('Bias/conv{}'.format(layer_id), layer.bias, \n",
    "#                                 epoch )\n",
    "            writer.add_histogram('Weight/conv{}'.format(layer_id), layer.weight, \n",
    "                                epoch )\n",
    "#             writer.add_histogram('Grad/conv{}'.format(layer_id), layer.weight.grad, \n",
    "#                                     epoch )\n",
    "            layer_id += 1\n",
    "\n",
    "\n",
    "def train ( train_loader, test_loader, net, dataset_size, my_device='cpu',\n",
    "           prefix=None, upper_bound=100.0, save=False, epochs=100, \n",
    "           lr=1e-1, device='cpu', debug=False, layers2tensorboard=False , batch_size=64) :\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(),lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    now = datetime.now()\n",
    "    suffix = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    prefix = suffix if prefix is None else prefix + '-' + suffix  \n",
    "\n",
    "    writer = SummaryWriter( log_dir=tensorboard_path+prefix )\n",
    "        \n",
    "    accuracies = []\n",
    "    max_accuracy = -1.0  \n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Training epochs...') :\n",
    "        net.train()\n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            train_x = train_x.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            predict_y = net( train_x )\n",
    "            \n",
    "            # Loss:\n",
    "            error = criterion( predict_y , train_label )\n",
    "\n",
    "            writer.add_scalar( 'Loss/train', error.cpu().item(), \n",
    "                                idx+( epoch*(dataset_size//batch_size) ) )\n",
    "\n",
    "            # Back propagation\n",
    "            optimizer.zero_grad()\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accuracy:\n",
    "            predict_ys = torch.max( predict_y, axis=1 )[1]\n",
    "            correct    = torch.sum(predict_ys == train_label)\n",
    "\n",
    "            writer.add_scalar( 'Accuracy/train', correct/train_x.size(0), \n",
    "                                idx+( epoch*(dataset_size//batch_size) ) )\n",
    "\n",
    "            if debug and idx % 10 == 0 :\n",
    "                print( f'idx: {idx:4d}, _error: {error.cpu().item():5.2f}' )\n",
    "\n",
    "        if layers2tensorboard :\n",
    "            plot_layers( net, writer, epoch )\n",
    "\n",
    "        accuracy = validate(net, test_loader, device=device)\n",
    "        accuracies.append(accuracy)\n",
    "        writer.add_scalar( 'Accuracy/test', accuracy, epoch )\n",
    "        \n",
    "        if accuracy > max_accuracy :\n",
    "            best_model = copy.deepcopy(net)\n",
    "            max_accuracy = accuracy\n",
    "            print(\"Saving Best Model with Accuracy: \", accuracy)\n",
    "        \n",
    "        print( f'Epoch: {epoch+1:3d} | Accuracy : {accuracy:7.4f}%' )\n",
    "\n",
    "        if accuracy > upper_bound :\n",
    "            break\n",
    "    \n",
    "    if save : \n",
    "        dataset = \"SoyNet\"\n",
    "        path = f'{models_path}ResNet18-{dataset}-{max_accuracy:.2f}.pkl'\n",
    "        torch.save(best_model, path)\n",
    "        print('Model saved in:',path)\n",
    "    \n",
    "    plt.plot(accuracies)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    return best_model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8076887",
   "metadata": {},
   "source": [
    "## Run with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dffcef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a683bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela  os parâmetros da ResNet18, exceto a última camada\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Modifica a última camada para o número de classes do dataset\n",
    "num_classes = 2\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee32cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: Frozen\n",
      "bn1.weight: Frozen\n",
      "bn1.bias: Frozen\n",
      "layer1.0.conv1.weight: Frozen\n",
      "layer1.0.bn1.weight: Frozen\n",
      "layer1.0.bn1.bias: Frozen\n",
      "layer1.0.conv2.weight: Frozen\n",
      "layer1.0.bn2.weight: Frozen\n",
      "layer1.0.bn2.bias: Frozen\n",
      "layer1.1.conv1.weight: Frozen\n",
      "layer1.1.bn1.weight: Frozen\n",
      "layer1.1.bn1.bias: Frozen\n",
      "layer1.1.conv2.weight: Frozen\n",
      "layer1.1.bn2.weight: Frozen\n",
      "layer1.1.bn2.bias: Frozen\n",
      "layer2.0.conv1.weight: Frozen\n",
      "layer2.0.bn1.weight: Frozen\n",
      "layer2.0.bn1.bias: Frozen\n",
      "layer2.0.conv2.weight: Frozen\n",
      "layer2.0.bn2.weight: Frozen\n",
      "layer2.0.bn2.bias: Frozen\n",
      "layer2.0.downsample.0.weight: Frozen\n",
      "layer2.0.downsample.1.weight: Frozen\n",
      "layer2.0.downsample.1.bias: Frozen\n",
      "layer2.1.conv1.weight: Frozen\n",
      "layer2.1.bn1.weight: Frozen\n",
      "layer2.1.bn1.bias: Frozen\n",
      "layer2.1.conv2.weight: Frozen\n",
      "layer2.1.bn2.weight: Frozen\n",
      "layer2.1.bn2.bias: Frozen\n",
      "layer3.0.conv1.weight: Frozen\n",
      "layer3.0.bn1.weight: Frozen\n",
      "layer3.0.bn1.bias: Frozen\n",
      "layer3.0.conv2.weight: Frozen\n",
      "layer3.0.bn2.weight: Frozen\n",
      "layer3.0.bn2.bias: Frozen\n",
      "layer3.0.downsample.0.weight: Frozen\n",
      "layer3.0.downsample.1.weight: Frozen\n",
      "layer3.0.downsample.1.bias: Frozen\n",
      "layer3.1.conv1.weight: Frozen\n",
      "layer3.1.bn1.weight: Frozen\n",
      "layer3.1.bn1.bias: Frozen\n",
      "layer3.1.conv2.weight: Frozen\n",
      "layer3.1.bn2.weight: Frozen\n",
      "layer3.1.bn2.bias: Frozen\n",
      "layer4.0.conv1.weight: Trainable\n",
      "layer4.0.bn1.weight: Trainable\n",
      "layer4.0.bn1.bias: Trainable\n",
      "layer4.0.conv2.weight: Trainable\n",
      "layer4.0.bn2.weight: Trainable\n",
      "layer4.0.bn2.bias: Trainable\n",
      "layer4.0.downsample.0.weight: Trainable\n",
      "layer4.0.downsample.1.weight: Trainable\n",
      "layer4.0.downsample.1.bias: Trainable\n",
      "layer4.1.conv1.weight: Trainable\n",
      "layer4.1.bn1.weight: Trainable\n",
      "layer4.1.bn1.bias: Trainable\n",
      "layer4.1.conv2.weight: Trainable\n",
      "layer4.1.bn2.weight: Trainable\n",
      "layer4.1.bn2.bias: Trainable\n",
      "fc.weight: Trainable\n",
      "fc.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    status = \"Trainable\" if param.requires_grad else \"Frozen\"\n",
    "    print(f\"{name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2eecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on {my_device.type}\")\n",
    "    \n",
    "model = model.to(my_device)\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-4\n",
    "dataset = 'flowers'\n",
    "prefix = 'AlexNet-TL-{}-e-{}-lr-{}'.format(dataset, epochs, lr)\n",
    "\n",
    "net = train(train_tensors, test_tensors, model, len(train_data),\n",
    "            epochs=epochs, device=my_device, save=False, \n",
    "            prefix=prefix, lr=lr, layers2tensorboard=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1edb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_and_predict ( net, seed=None ) :\n",
    "\n",
    "    if seed is not None :\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    dataset = ImageFolder(root=dataset_path, transform=None)\n",
    "\n",
    "    data = train_test_dataset(dataset, 0.30)[1]\n",
    "    \n",
    "    my_transform = ResNet18_Weights.DEFAULT.transforms()\n",
    "    i = np.random.randint(len(data))\n",
    "    \n",
    "    sample = data[i][0]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.axis('off')\n",
    "    plt.imshow( sample )\n",
    "\n",
    "    print( f'Sample id: {i:3d}' )\n",
    "    \n",
    "    x = my_transform(sample)\n",
    "    print(x.shape)\n",
    "\n",
    "    x = x.unsqueeze_(0)\n",
    "    print(x.shape)\n",
    "\n",
    "    x = x.to(my_device)\n",
    "    \n",
    "    output = net ( x )\n",
    "    predictions = output.squeeze(0).softmax(0)\n",
    "        \n",
    "    predicted_class = torch.argmax(predictions)\n",
    "    predicted_class = predicted_class.data.cpu().item()\n",
    "    \n",
    "    confidence = predictions[predicted_class]\n",
    "    confidence = confidence.data.cpu().item()\n",
    "    \n",
    "    dataset_classes = [\"Disease\", \"Healthy\"]\n",
    "\n",
    "    if predicted_class == data[i][1] : print('Hit')\n",
    "    else: print('Miss')\n",
    "\n",
    "    print(predicted_class)\n",
    "    print( f'Predicted: {dataset_classes[predicted_class]} | Corrected: {dataset_classes[data[i][1]]} | Confidence: {confidence*100:.2f}%'  )\n",
    "    \n",
    "    # return dataset_classes[y], dataset_classes[data[i][1]], confidence\n",
    "\n",
    "sample_and_predict(net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ambiente-hLVTNKm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
